---
title: "county exploration"
output: github_document
---

libraries
```{r}
library(tidyverse)
library(dplyr)
```

read in participation data
```{r}
# Load required library
library(dplyr)

# Read in the datasets
fbi_participation_2023 <- read.csv("C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\FBI Agency Participation Data\\2023 Participation.csv")
part_ref_crosswalk <- read.csv("C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\FBI Agency Participation Data\\part_ref_crosswalk.csv")

# Perform the merge
merged_df <- fbi_participation_2023 %>%
  left_join(part_ref_crosswalk, by = "city_state")

# Replace city_state in fbi_participation_2023 with crosswalk_name where the merge is successful
fbi_participation_2023_updated <- merged_df %>%
  mutate(city_state = ifelse(!is.na(crosswalk_name), crosswalk_name, city_state)) %>%
  select(-crosswalk_name)  # Remove the crosswalk_name column if not needed

# Save the updated dataframe to a new CSV file
write.csv(fbi_participation_2023_updated, "C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\FBI Agency Participation Data\\2023_Participation_Updated.csv", row.names = FALSE)

print("The updated file has been saved as '2023_Participation_Updated.csv'.")

```
subset to counties with >100k pop
```{r}
cities_40k <- fbi_participation_2023_updated %>% 
  filter(agency_type_name == "City", population > 40000)

counties_100k <- fbi_participation_2023 %>% 
  filter(agency_type_name == "County", state_abbr == "TN", population > 100000)

# Count unique county entities
# Count unique county entities based on ncic_agency_name and state_abbr
num_unique_agencies <- counties_100k %>% 
  distinct(ncic_agency_name, state_abbr) %>% 
  nrow()

# Create a DataFrame containing all duplicates
duplicates_df <- counties_100k %>% 
  group_by(ncic_agency_name, state_abbr) %>% 
  filter(n() > 1) %>% 
  ungroup()

# Select additional columns (example: pub_agency_name, population, year)
counties_100k_reference <- counties_100k %>% 
  select(ncic_agency_name, pub_agency_name, state_name, state_abbr, population)

# Write the resulting DataFrame to a CSV file
output_file <- "C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\cities_40k.csv"
write.csv(cities_40k, output_file, row.names = FALSE)
```
december 2024 sample prepare
```{r}
read in the data
```{r}
library(tidyverse)
library(magrittr)
library(dplyr)
library(readr)
library(stringr)
library(zoo)
library(ggplot2)
library(here)
```
directory path where the data files are stored
UPDATE MONTH on LINE 44!
```{r}
#directory path where the data files are stored
# Dynamically locate OneDrive directory
onedrive_mac <- "~/Library/CloudStorage/OneDrive-ahdatalytics.com"
onedrive_win <- file.path("C:", "OneDrive", "OneDrive - ahdatalytics.com")

if (dir.exists(onedrive_mac)) {
  onedrive_dir <- normalizePath(onedrive_mac)
} else if (dir.exists(onedrive_win)) {
  onedrive_dir <- normalizePath(onedrive_win)
} else {
  stop("OneDrive directory not found.")
}

# Define the raw data directory with the correct relative path from OneDrive
data_dir <- file.path(
  onedrive_dir,
  "Clients",
  "Real Time Crime Index",
  "Open Source Data",
  "_Ben and Jeff work",
  "Counties"
)

# Check the full path to ensure it's correct
print(data_dir)

# Get a list of all CSV file paths within the directory
file_paths <- list.files(data_dir, pattern = "\\.csv$", full.names = TRUE)

# Check the files
head(file_paths)
```
read in counties files
```{r}
library(readxl)
library(tools)

# Function to read the first sheet from all Excel files in a folder and save them as data frames in a list
read_excels <- function(folder_path) {
  # List all Excel files in the directory
  file_paths <- list.files(folder_path, pattern = "\\.xlsx$", full.names = TRUE)
  
  # Debug: Check if any files are listed
  if (length(file_paths) == 0) {
    stop("No Excel files found in the specified directory.")
  } else {
    message("Processing ", length(file_paths), " files.")
  }
  
  # Function to read the first sheet of a single Excel file
  read_excel_file <- function(file_path) {
    # Debug: Check the file path
    message("Reading file: ", file_path)
    
    # Attempt to read the first sheet of the Excel file
    tryCatch({
      df <- read_excel(file_path, sheet = 1)
      return(df)
    }, error = function(e) {
      warning("Failed to read: ", file_path, " Error: ", e$message)
      return(NULL)  # Return NULL if the file cannot be read
    })
  }
  
  # Read all Excel files and save them as data frames in a list
  df_list <- lapply(file_paths, read_excel_file)
  
  # Name each element in the list with the corresponding file name (without extension)
  names(df_list) <- file_path_sans_ext(basename(file_paths))
  
  return(df_list)
}

# Read Excel files from the directory
ldf <- read_excels(data_dir)

# View the first data frame in the list
head(ldf[[1]])
```
let's get the unique number of agencies at the beginning
```{r}
# Function to get the count of unique 'Agency Name' in each csv
get_unique_agency_count <- function(df) {
  length(unique(df$`Agency Name`))
}

# Apply the function to each dataframe in the list and store the results
unique_agency_counts <- map(ldf, get_unique_agency_count)
total_agency_raw <- sum(unlist(unique_agency_counts))
head(total_agency_raw)
##393 for November 2024
```
format everything
```{r}
# Define the format_cols function
format_cols <- function(df) {
  # Remove the Arson column
  df <- df[, !(names(df) %in% "Arson")]
  
  # Convert specified columns to character
  char_cols <- c("Agency Name", "State")
  for (col in char_cols) {
    if (col %in% names(df)) {
      df[[col]] <- as.character(df[[col]])
    }
  }
  
  # Convert specified columns to numeric
  num_cols <- c("Murder", "Rape", "Robbery", "Aggravated Assault", "Burglary",
                "Theft", "Motor Vehicle Theft", "Month", "Year")
  for (col in num_cols) {
    if (col %in% names(df)) {
      df[[col]] <- as.numeric(df[[col]])
    }
  }
  
  return(df)
}

# Apply to all dataframes in the list
ldf1 <- lapply(ldf, format_cols)

# What do we see
head(ldf1[1])
```
post-formatting check we still have the same number of agencies before binding
```{r}
# Apply the function to each dataframe in the list and store the results
unique_agency_counts1 <- map(ldf1, get_unique_agency_count)

# what is the sum of all those raw counts
total_agency_raw1 <- sum(unlist(unique_agency_counts1))

# Print the results
head(total_agency_raw1)
#matches up - November 2024
```
next...standardize the columns across all dataframes in the list
```{r}
# Ensure all data frames have the same columns and column types
all_columns <- Reduce(union, lapply(ldf1, names))

# Function to ensure all data frames have the same columns
standardize_columns <- function(df, all_columns) {
  missing_cols <- setdiff(all_columns, names(df))
  for (col in missing_cols) {
    df[[col]] <- NA
  }
  return(df[all_columns])
}

# Apply the standardize_columns function to each dataframe in the list
ldf2 <- lapply(ldf1, standardize_columns, all_columns = all_columns)

#make sure we're good
head(ldf2[1])
```
another check that after formatting we still have the same number of agencies before binding
```{r}
# Apply the function to each dataframe in the list and store the results
unique_agency_counts2 <- map(ldf2, get_unique_agency_count)

# what is the sum of all those raw counts
total_agency_raw2 <- sum(unlist(unique_agency_counts2))

# Print the results
head(total_agency_raw2)
#393 - matches up November
```
get rid of the extra blank lgl formatted columns that plague our data
```{r}
acceptable_names <- c("Agency Name", "Murder", "Rape", "Robbery", "Aggravated Assault", 
"Burglary", "Theft", "Motor Vehicle Theft", "Year", "Month", "State")

#remove any columns not in the above list
ldf3 <- lapply(ldf2, function(df) {
  df[, (names(df) %in% acceptable_names)]
})

#take a look
head(ldf3[1])
```
one last check that after formatting & standardizing we still have the same number of agencies before binding
```{r}
# Apply the function to each dataframe in the list and store the results
unique_agency_counts3 <- map(ldf3, get_unique_agency_count)

# what is the sum of all those raw counts
total_agency_raw3 <- sum(unlist(unique_agency_counts3))

# check it out
print(total_agency_raw3)
#393 matches - November
```
stop the process if there is a dataframe that does not have each of the acceptable names and print
```{r}
# Initialize an empty list to store the names of dataframes with missing columns
df_missing_columns <- list()

# Function to check if a dataframe has all the acceptable column names
check_columns <- function(df, df_name, acceptable_names) {
  missing_cols <- setdiff(acceptable_names, colnames(df))
  if (length(missing_cols) > 0) {
    df_missing_columns[[df_name]] <- missing_cols  # Add to the list if columns are missing
  }
}

# Loop through the dataframes and check each one
for (df_name in names(ldf3)) {
  check_columns(ldf3[[df_name]], df_name, acceptable_names)
}

# Review the dataframes that are missing columns
if (length(df_missing_columns) > 0) {
  print("The following dataframes are missing columns:")
  print(df_missing_columns)
} else {
  print("All dataframes have the required columns.")
}
```
combine and check it out
```{r}
# combine everything together 
combined_df <- bind_rows(ldf3)

# Step 2: Reset row names
rownames(combined_df) <- NULL

#select out acceptable columns and take a look just to make sure
combined_df <- combined_df %>% select(acceptable_names)
head(combined_df)
#looking good.
```
city state column for getting unique number of agencies
```{r}
#create the city_state column here for a unique ID
combined_df <- combined_df %>% mutate(city_state = paste(`Agency Name`, State, sep = ", "))

#check that unique agencies checks matches city_state check
length(unique(combined_df$city_state))
#393 all set
length(unique(combined_df$`Agency Name`))
#380 - springfields galore!
```
check out city_state by state for a quick numbers check
```{r}
library(dplyr)

# Example: Count unique city_state values per State
agency_by_state <- combined_df %>%
  group_by(State) %>%
  summarise(unique_city_state_count = n_distinct(city_state))

na_city_states <- combined_df %>%
  filter(is.na(State)) %>%
  select(city_state)

# View the result
print(na_city_states)
```
final count issues ID'ed before sent along to be merged with base dataset
```{r}
# Ensure NA's are explicitly marked as NA in designated columns
cdf <- combined_df %>%
  mutate(
    Murder = ifelse(is.na(Murder), NA, Murder),
    Rape = ifelse(is.na(Rape), NA, Rape),
    Robbery = ifelse(is.na(Robbery), NA, Robbery),
    `Aggravated Assault`= ifelse(is.na(`Aggravated Assault`), NA, `Aggravated Assault`),
    Burglary = ifelse(is.na(Burglary), NA, Burglary),
    Theft = ifelse(is.na(Theft), NA, Theft),
    `Motor Vehicle Theft` = ifelse(is.na(`Motor Vehicle Theft`),NA, `Motor Vehicle Theft`)
  )
```