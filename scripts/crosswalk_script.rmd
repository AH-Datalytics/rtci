---
title: "linking historical ucr data"
output: github_document
---
libraries
```{r}
library(tidyverse)
library(magrittr)
library(dplyr)
library(readr)
library(stringr)
library(zoo)
library(ggplot2)
library(here)
library(purrr)
```
dynamically find one drive
```{r}
# Dynamically locate OneDrive directory
onedrive_mac <- "~/Library/CloudStorage/OneDrive-ahdatalytics.com"
onedrive_win <- file.path("C:", "OneDrive", "OneDrive - ahdatalytics.com")

if (dir.exists(onedrive_mac)) {
  onedrive_dir <- normalizePath(onedrive_mac)
} else if (dir.exists(onedrive_win)) {
  onedrive_dir <- normalizePath(onedrive_win)
} else {
  stop("OneDrive directory not found.")
}
```
read in fbi participation file
```{r}
fbi_participation_2023 <- read.csv("C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\FBI Agency Participation Data\\2023 Participation_adapted.csv")
part_ref_crosswalk <- read.csv("C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\FBI Agency Participation Data\\part_ref_crosswalk.csv")
```
let's look at 2024
```{r}
# ---- Setup ----
# Install once if needed:
# install.packages(c("readxl", "dplyr", "janitor", "purrr", "fs"))

library(readxl)
library(dplyr)
library(janitor)
library(purrr)
library(fs)

# Windows paths in R work best with double backslashes \\ or forward slashes /.
xlsx_path <- "C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\_Ben and Jeff work\\Ben and Jeff working files\\2024 Population File.xlsx"

# ---- 1) Check the file exists ----
if (!fs::file_exists(xlsx_path)) {
  stop("File not found. Check the path or that OneDrive is synced locally:\n", xlsx_path)
}

# ---- 2) List sheet names (helpful when the workbook has multiple tabs) ----
sheets <- readxl::excel_sheets(xlsx_path)
message("Sheets found: ", paste(sheets, collapse = " | "))

# ---- 3a) Read a specific sheet (edit 'sheet_to_read' if you know the tab name) ----
# If you're not sure, look at the message above and set the name or index below.
sheet_to_read <- sheets[1]   # e.g., "Sheet1" or an index like 1

population_df <- readxl::read_excel(
  path = xlsx_path,
  sheet = sheet_to_read,
  # col_types = "guess"  # uncomment to control types; or supply a vector
) %>%
  janitor::clean_names()  # lower_snake_case, removes spaces/symbols

# Quick peek
print(paste0("Rows: ", nrow(population_df), " | Cols: ", ncol(population_df)))
glimpse(population_df)

# ---- 3b) (Optional) Read ALL sheets into a named list ----
# Each element is a tibble for that sheet, with cleaned column names.
population_lst <- purrr::map(
  .x = sheets,
  .f = ~ readxl::read_excel(xlsx_path, sheet = .x) %>% janitor::clean_names()
)
names(population_lst) <- sheets

# Example: access a specific tab after loading all
# population_lst[["SheetName"]]

# ---- 4) (Optional) Save out as CSV for reproducibility ----
# write.csv(population_df, "C:\\Users\\daveh\\OneDrive\\Desktop\\2024_population_file.csv", row.names = FALSE)
```
merge, clean up and pare down df's before fixing place names
```{r}
fbi_participation_2023_updated <- fbi_participation_2023 %>%
  mutate(
  city_state = paste(pub_agency_name, state_abbr, sep = " ,")) %>%
  select(ori, pub_agency_name, state_abbr, city_state, region_name, division_name, agency_type_name, population)

# merge 
#crosswalked_df <- fbi_participation_2023_updated %>%
  #left_join(part_ref_crosswalk, by = c("city_state" = "fbi_name"))


# Replace city_state in fbi_participation_2023 with crosswalk_name where the merge is successful
#fbi_participation_2023_updated <- cross_walked_df %>%
  #mutate(city_state = ifelse(!is.na(city_state.y), city_state.y, city_state)) %>%
  #select(-city_state.y) 
  
fbi_participation_2023_updated <- fbi_participation_2023_updated %>% mutate(city_state_id = paste(pub_agency_name, state_abbr, agency_type_name, sep = ","))  # Remove the crosswalk_name column if not needed
```
merge 2024 to 2023 based on ori
```{r}
library(dplyr)
#fix ori columns
population_df <- population_df %>% select(-ori)

population_df <- population_df %>% rename(ori = ori9)

# Ensure ori column exists in both dataframes and is the same case
population_df <- population_df %>%
  mutate(ori = toupper(ori))

fbi_participation_2023_updated <- fbi_participation_2023_updated %>%
  mutate(ori = toupper(ori))

# ---- LEFT JOIN: Keep all records from fbi_participation_2023_updated ----
merged_df_left <- fbi_participation_2023_updated %>%
  left_join(population_df, by = "ori")

# ---- INNER JOIN: Keep only matching ori in both ----
merged_df_inner <- fbi_participation_2023_updated %>%
  inner_join(population_df, by = "ori")

# ---- Quick checks ----
message("Left join rows: ", nrow(merged_df_left))
message("Inner join rows: ", nrow(merged_df_inner))

# Optional: Preview
glimpse(merged_df_left)
```
look at pop differences
```{r}
merged_df_left <- merged_df_left %>% mutate(pop_diff = (population - population_1))

#update population_1 
merged_df_left <- merged_df_left %>%
  mutate(population_1 = coalesce(population_1, population))
```

subset to counties with >100k pop
```{r}
cities_40k <- merged_df_left %>% 
  filter(agency_type_name == "City", population.x > 30000)

counties_100k <- merged_df_left %>% 
  filter(agency_type_name == "County", population.x > 100000)
```
combine city and county records for merging with final_sourcing 
```{r}
cities_counties_ref <- bind_rows(cities_40k, counties_100k)
cities_counties_ref <- cities_counties_ref  %>% mutate(Agency_Type = agency_type_name) %>% select(-agency_type_name)
```
add population figures for nationwide descriptors
```{r}
# Define the path to the ref_df file
#ref_df_path <- file.path("C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\Collected Sample Data\\final_sourcing.csv")

# Define the reference data file path relative to the base OneDrive directory
ref_df_path <- file.path(
  onedrive_dir,
  "Clients",
  "Real Time Crime Index",
  "Open Source Data",
  "Collected Sample Data",
  "final_sourcing.csv"
)

# Print the reference data file path to confirm
print(ref_df_path)
# Read in the ref_df file
ref_df <- read.csv(ref_df_path)

# Make a city_state column for matching
ref_df <- ref_df %>% mutate(city_state_id = paste(Agency, State, Agency_Type, sep = ","))
```
add region name to final sourcing table
```{r}
# Merge with ref_df by "city_state"
ref_df1 <- merge(ref_df, cities_counties_ref, by = "city_state_id", all.x = TRUE)

# Check if each record in ref_df has the region_name column merged successfully
check_region <- ref_df1 %>%
  mutate(successful_merge = !is.na(region_name))

# Print a summary of merge success
summary_merge <- check_region %>%
  summarize(
    total_records = n(),
    successful_merges = sum(successful_merge),
    failed_merges = sum(!successful_merge)
  )

# Display the summary
print(summary_merge)

# Inspect rows with failed merges, if any
failed_merges <- check_region %>%
  filter(!successful_merge)

if (nrow(failed_merges) > 0) {
  print("The following records failed to merge successfully:")
  print(failed_merges)
} else {
  print("All records were merged successfully.")
}

#remove excess columns
#write.csv(failed_merges, "failed_merges.csv", row.names = FALSE)
```
create city state field and drop the additional
```{r}
ref_df1 <- ref_df1 %>% mutate(city_state = city_state.x) %>% select(-city_state.y, -city_state.x, -Agency_Type.y)
```
run above - also check to make sure we have everything we need
```{r}
colSums(is.na(ref_df1)
)

#fix Agency_Type column name and drop Population
ref_df1<- ref_df1 %>% mutate(Agency_Type = Agency_Type.x) %>% select(-Agency_Type.x, -Population)

#make sure they are the correct types
str(ref_df1)

colSums(is.na(ref_df1))
```
let's investigate population vs. pop23
```{r}
ref_df1 <- ref_df1 %>% mutate(pop_diff = pop23 - population)
#Everett, MA = -1400
#Dublin, OH = -28
#Molie, IL = 408
#Lakewood, OH = 186
#Euclid, OH = 174
#York, PA = 25
```
missing: hamilton township, NJ; henry, ga, county; st mary's, md county



updating sample division of labor
```{r}
# Packages
library(readr)
library(dplyr)

# --- 1) Read files ---
# Tip: Windows paths work fine with forward slashes
sample_cities <- read_csv("C:/Users/daveh/Downloads/sample_cities (1).csv", show_col_types = FALSE)
aggregated     <- read_csv("C:/Users/daveh/Downloads/aggregated (4).csv", show_col_types = FALSE)

# Expectation:
# - ref_df1 already exists in your environment and has at least: city_state_id, ori (and possibly more)
# - sample_cities has city_state_id (and possibly more)
# - aggregated has ori (and possibly more)

# --- 2) Merge sample_cities with ref_df1 on city_state_id; keep only ori and city_state_id ---
# Using inner_join to keep only matched city_state_id rows present in both data frames
sample_cities_ref <- sample_cities %>%
  inner_join(ref_df1, by = "city_state_id") %>%
  transmute(ori, city_state_id) %>%
  distinct() %>%
  filter(!is.na(ori), !is.na(city_state_id))

# --- 3) Build a second df: unique ORIs from aggregated ---
aggregated_oris <- aggregated %>%
  select(name, ori) %>%
  filter(!is.na(ori)) %>%
  group_by(ori) %>%
  slice(1) %>%
  ungroup()


# --- 4) Compare and produce the two requested outputs ---
# (a) All ORIs found in BOTH (keep city_state_id from the sample_cities/ref join)
oris_in_both <- sample_cities_ref %>%
  semi_join(aggregated_oris, by = "ori")
# -> columns: ori, city_state_id (one row per ori-city_state_id present in both)

# (b) All ORIs found SOLELY in the aggregated fileâ€™s df
oris_only_in_aggregated <- aggregated_oris %>%
  anti_join(sample_cities_ref, by = "ori")
# -> columns: ori (these appear in aggregated but not in the sample_cities/ref join)

# Optional: quick counts to sanity check
cat("n(sample_cities_ref): ", nrow(sample_cities_ref), "\n")
cat("n(unique aggregated ORIs): ", nrow(aggregated_oris), "\n")
cat("n(ORIs in both): ", n_distinct(oris_in_both$ori), "\n")
cat("n(ORIs only in aggregated): ", nrow(oris_only_in_aggregated), "\n")

```
