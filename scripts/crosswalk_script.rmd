---
title: "linking historical ucr data"
output: github_document
---
libraries
```{r}
library(tidyverse)
library(magrittr)
library(dplyr)
library(readr)
library(stringr)
library(zoo)
library(ggplot2)
library(here)
library(purrr)
```
dynamically find one drive
```{r}
# Dynamically locate OneDrive directory
onedrive_mac <- "~/Library/CloudStorage/OneDrive-ahdatalytics.com"
onedrive_win <- file.path("C:", "OneDrive", "OneDrive - ahdatalytics.com")

if (dir.exists(onedrive_mac)) {
  onedrive_dir <- normalizePath(onedrive_mac)
} else if (dir.exists(onedrive_win)) {
  onedrive_dir <- normalizePath(onedrive_win)
} else {
  stop("OneDrive directory not found.")
}
```
read in fbi participation file
```{r}
fbi_participation_2023 <- read.csv("C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\FBI Agency Participation Data\\2023 Participation_adapted.csv")
part_ref_crosswalk <- read.csv("C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\FBI Agency Participation Data\\part_ref_crosswalk.csv")
```
let's look at 2024
```{r}
# ---- Setup ----
# Install once if needed:
# install.packages(c("readxl", "dplyr", "janitor", "purrr", "fs"))

library(readxl)
library(dplyr)
library(janitor)
library(purrr)
library(fs)

# Windows paths in R work best with double backslashes \\ or forward slashes /.
xlsx_path <- "C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\_Ben and Jeff work\\Ben and Jeff working files\\2024 Population File.xlsx"

# ---- 1) Check the file exists ----
if (!fs::file_exists(xlsx_path)) {
  stop("File not found. Check the path or that OneDrive is synced locally:\n", xlsx_path)
}

# ---- 2) List sheet names (helpful when the workbook has multiple tabs) ----
sheets <- readxl::excel_sheets(xlsx_path)
message("Sheets found: ", paste(sheets, collapse = " | "))

# ---- 3a) Read a specific sheet (edit 'sheet_to_read' if you know the tab name) ----
# If you're not sure, look at the message above and set the name or index below.
sheet_to_read <- sheets[1]   # e.g., "Sheet1" or an index like 1

population_df <- readxl::read_excel(
  path = xlsx_path,
  sheet = sheet_to_read,
  # col_types = "guess"  # uncomment to control types; or supply a vector
) %>%
  janitor::clean_names()  # lower_snake_case, removes spaces/symbols

# Quick peek
print(paste0("Rows: ", nrow(population_df), " | Cols: ", ncol(population_df)))
glimpse(population_df)

# ---- 3b) (Optional) Read ALL sheets into a named list ----
# Each element is a tibble for that sheet, with cleaned column names.
population_lst <- purrr::map(
  .x = sheets,
  .f = ~ readxl::read_excel(xlsx_path, sheet = .x) %>% janitor::clean_names()
)
names(population_lst) <- sheets

# Example: access a specific tab after loading all
# population_lst[["SheetName"]]

# ---- 4) (Optional) Save out as CSV for reproducibility ----
# write.csv(population_df, "C:\\Users\\daveh\\OneDrive\\Desktop\\2024_population_file.csv", row.names = FALSE)
```
merge, clean up and pare down df's before fixing place names
```{r}
fbi_participation_2023_updated <- fbi_participation_2023 %>%
  mutate(
  city_state = paste(pub_agency_name, state_abbr, sep = " ,")) %>%
  select(ori, pub_agency_name, state_abbr, city_state, region_name, division_name, agency_type_name, population)

# merge 
#crosswalked_df <- fbi_participation_2023_updated %>%
  #left_join(part_ref_crosswalk, by = c("city_state" = "fbi_name"))


# Replace city_state in fbi_participation_2023 with crosswalk_name where the merge is successful
#fbi_participation_2023_updated <- cross_walked_df %>%
  #mutate(city_state = ifelse(!is.na(city_state.y), city_state.y, city_state)) %>%
  #select(-city_state.y) 
  
fbi_participation_2023_updated <- fbi_participation_2023_updated %>% mutate(city_state_id = paste(pub_agency_name, state_abbr, agency_type_name, sep = ","))  # Remove the crosswalk_name column if not needed
```
merge 2024 to 2023 based on ori
```{r}
library(dplyr)
#fix ori columns
population_df <- population_df %>% select(-ori)

population_df <- population_df %>% rename(ori = ori9)

# Ensure ori column exists in both dataframes and is the same case
population_df <- population_df %>%
  mutate(ori = toupper(ori))

fbi_participation_2023_updated <- fbi_participation_2023_updated %>%
  mutate(ori = toupper(ori))

# ---- LEFT JOIN: Keep all records from fbi_participation_2023_updated ----
merged_df_left <- fbi_participation_2023_updated %>%
  left_join(population_df, by = "ori")

# ---- INNER JOIN: Keep only matching ori in both ----
merged_df_inner <- fbi_participation_2023_updated %>%
  inner_join(population_df, by = "ori")

# ---- Quick checks ----
message("Left join rows: ", nrow(merged_df_left))
message("Inner join rows: ", nrow(merged_df_inner))

# Optional: Preview
glimpse(merged_df_left)
```
look at pop differences
```{r}
merged_df_left <- merged_df_left %>% mutate(pop_diff = (population - population_1))

#update population_1 
merged_df_left <- merged_df_left %>%
  mutate(population_1 = coalesce(population_1, population))

#drop city_state.y and population.y
#rename city_state.x and population.x

merged_df_left <- merged_df_left %>%
  select(-city_state.y, -population.y) %>%
  mutate(city_state = city_state.x,
          population = population.x) %>%
          select(-city_state.x, -population.x)

names(merged_df_left)
```

subset to counties with >100k pop
```{r}
cities_40k <- merged_df_left %>% 
  filter(agency_type_name == "City", population > 30000)

counties_100k <- merged_df_left %>% 
  filter(agency_type_name == "County", population > 100000)
```
combine city and county records for merging with final_sourcing 
```{r}
cities_counties_ref <- bind_rows(cities_40k, counties_100k)
cities_counties_ref <- cities_counties_ref  %>% mutate(Agency_Type = agency_type_name) %>% select(-agency_type_name)
```
add population figures for nationwide descriptors
```{r}
# Define the path to the ref_df file
#ref_df_path <- file.path("C:\\OneDrive\\OneDrive - ahdatalytics.com\\Clients\\Real Time Crime Index\\Open Source Data\\Collected Sample Data\\final_sourcing.csv")

# Define the reference data file path relative to the base OneDrive directory
ref_df_path <- file.path(
  onedrive_dir,
  "Clients",
  "Real Time Crime Index",
  "Open Source Data",
  "Collected Sample Data",
  "final_sourcing.csv"
)

# Print the reference data file path to confirm
print(ref_df_path)
# Read in the ref_df file
ref_df <- read.csv(ref_df_path)

# Make a city_state column for matching
ref_df <- ref_df %>% mutate(city_state_id = paste(Agency, State, Agency_Type, sep = ","))
```
add region name to final sourcing table
```{r}
# Merge with ref_df by "city_state"
ref_df1 <- merge(ref_df, cities_counties_ref, by = "city_state_id", all.x = TRUE)

# Check if each record in ref_df has the region_name column merged successfully
check_region <- ref_df1 %>%
  mutate(successful_merge = !is.na(region_name))

# Print a summary of merge success
summary_merge <- check_region %>%
  summarize(
    total_records = n(),
    successful_merges = sum(successful_merge),
    failed_merges = sum(!successful_merge)
  )

# Display the summary
print(summary_merge)

# Inspect rows with failed merges, if any
failed_merges <- check_region %>%
  filter(!successful_merge)

if (nrow(failed_merges) > 0) {
  print("The following records failed to merge successfully:")
  print(failed_merges)
} else {
  print("All records were merged successfully.")
}

#remove excess columns
#write.csv(failed_merges, "failed_merges.csv", row.names = FALSE)
```
create city state field and drop the additional
```{r}
ref_df1 <- ref_df1 %>% mutate(city_state = city_state.x) %>% select(-city_state.y, -city_state.x, -Agency_Type.y)
```
run above - also check to make sure we have everything we need
```{r}
colSums(is.na(ref_df1)
)

#fix Agency_Type column name and drop Population
ref_df1<- ref_df1 %>% mutate(Agency_Type = Agency_Type.x) %>% select(-Agency_Type.x, -Population)

#make sure they are the correct types
str(ref_df1)

colSums(is.na(ref_df1))
```
let's investigate population vs. pop23
```{r}
ref_df1 <- ref_df1 %>% mutate(pop_diff = pop23 - population)
#Everett, MA = -1400
#Dublin, OH = -28
#Molie, IL = 408
#Lakewood, OH = 186
#Euclid, OH = 174
#York, PA = 25
```
missing: hamilton township, NJ; henry, ga, county; st mary's, md county



updating sample division of labor
```{r}
# Packages
library(readr)
library(dplyr)

# --- 1) Read files ---
# Tip: Windows paths work fine with forward slashes
sample_cities <- read_csv("C:/Users/daveh/Downloads/sample_cities (2).csv", show_col_types = FALSE)
aggregated     <- read_csv("C:/Users/daveh/Downloads/cde_data_since_1985 (2).csv", show_col_types = FALSE)

# Expectation:
# - ref_df1 already exists in your environment and has at least: city_state_id, ori (and possibly more)
# - sample_cities has city_state_id (and possibly more)
# - aggregated has ori (and possibly more)

# --- 2) Merge sample_cities with ref_df1 on city_state_id; keep only ori and city_state_id ---
# Using inner_join to keep only matched city_state_id rows present in both data frames
sample_cities_ref <- sample_cities %>%
  inner_join(ref_df1, by = "city_state_id") %>%
  transmute(ori, city_state_id) %>%
  distinct() %>%
  filter(!is.na(ori), !is.na(city_state_id))

# --- 3) Build a second df: unique ORIs from aggregated ---
aggregated_oris <- aggregated %>%
  select(name, ori) %>%
  filter(!is.na(ori)) %>%
  group_by(ori) %>%
  slice(1) %>%
  ungroup()


# --- 4) Compare and produce the two requested outputs ---
# (a) All ORIs found in BOTH (keep city_state_id from the sample_cities/ref join)
oris_in_both <- sample_cities_ref %>%
  semi_join(aggregated_oris, by = "ori")
# -> columns: ori, city_state_id (one row per ori-city_state_id present in both)

# (b) All ORIs found SOLELY in the aggregated fileâ€™s df
oris_only_in_aggregated <- aggregated_oris %>%
  anti_join(sample_cities_ref, by = "ori")
# -> columns: ori (these appear in aggregated but not in the sample_cities/ref join)

# Optional: quick counts to sanity check
cat("n(sample_cities_ref): ", nrow(sample_cities_ref), "\n")
cat("n(unique aggregated ORIs): ", nrow(aggregated_oris), "\n")
cat("n(ORIs in both): ", n_distinct(oris_in_both$ori), "\n")
cat("n(ORIs only in aggregated): ", nrow(oris_only_in_aggregated), "\n")

```
write it out
```{r}
# Define file path
output_path <- "C:/OneDrive/OneDrive - ahdatalytics.com/Clients/Real Time Crime Index/Open Source Data/Collected Sample Data/check_agencies.csv"

# Write to CSV
write.csv(oris_only_in_aggregated, output_path, row.names = FALSE)

```

grab data from cde since 1985 to review
```{r}
# Load packages
library(readr)
library(dplyr)
library(stringr)

# 1) Read the two files (use forward slashes on Windows to avoid escaping)
new_cde_agencies <- read_csv(
  "C:/OneDrive/OneDrive - ahdatalytics.com/Clients/Real Time Crime Index/Open Source Data/Collected Sample Data/check_agencies.csv",
  show_col_types = FALSE
)

cde_data_all <- read_csv(
  "C:/Users/daveh/Downloads/cde_data_since_1985 (2).csv",
  show_col_types = FALSE
)

# 2) From new_cde_agencies, keep every unique ORI 
new_cde_oris <- new_cde_agencies %>%
  select(ori)    # drop blanks just in case

# 3) Filter the second df:
#    - year >= 2017
#    - ori appears in the Y-list above
cde_data_filtered <- cde_data_all %>%
  mutate(year = as.integer(year)) %>%    # coerce year safely
  filter(year >= 2017) %>%
  semi_join(new_cde_oris, by = "ori")

# (Optional sanity checks)
nrow(new_cde_oris)         # how many ORIs flagged Y
n_distinct(cde_data_filtered$ori)  # how many ORIs remain in filtered data


```
write it out
```{r}
# Define output path
output_path <- "C:/OneDrive/OneDrive - ahdatalytics.com/Clients/Real Time Crime Index/Open Source Data/Collected Sample Data/cde_data_filtered.csv"

# Write out as CSV
write_csv(cde_data_filtered, output_path)


```
create data for review
```{r}
#read in edited csv


#filter to only those records where yn == y


#write it out for jeff to review for inclusion into main sample

# Load libraries
library(dplyr)
library(readr)

# Define paths
input_path <- "C:/OneDrive/OneDrive - ahdatalytics.com/Clients/Real Time Crime Index/Open Source Data/Collected Sample Data/check_agencies_aug.csv"
output_path <- "C:/OneDrive/OneDrive - ahdatalytics.com/Clients/Real Time Crime Index/Open Source Data/Collected Sample Data/agencies_inclusion_review.csv"

# Read the CSV
check_agencies <- read_csv(input_path, show_col_types = FALSE)

# Filter records where 'yn' column == 'y' (case-insensitive)
agencies_inclusion_review <- check_agencies %>%
  filter(tolower(yn) == "y")

# Write filtered results back to the same folder
write_csv(agencies_inclusion_review, output_path)

# Confirmation message
cat("Filtered file written to:", output_path, "\n")


````









check out the CA agencies and grab the ca doj data from filtered
```{r}
library(readr)
library(dplyr)
library(stringr)

# 1) Read in the CA DOJ data
ca_doj_data <- read_csv(
  "C:/Users/daveh/Downloads/Crimes_and_Clearances_with_Arson-1985-2024_by month.csv",
  show_col_types = FALSE
)

# 2) Filter new_cde_agencies to find ORIs starting with "CA"
ca_agencies_new <- new_cde_agencies %>%
  filter(str_sub(ori, 1, 2) == "CA")

ca_agencies_new <- ca_agencies_new %>%
  filter(include_yn == "Y")

# Quick check
nrow(ca_agencies_new)          # number of CA agencies flagged
head(ca_agencies_new$ori, 10)  # preview ORIs

```
use fuzzy matching to find matching name candidates
```{r}
# Packages
library(dplyr)
library(stringr)
library(fuzzyjoin)
library(readr)

#--- assume you already have:
# ca_agencies_new (with columns: ori, name, etc.)
# ca_doj_data (with column: NCICCode)
# If not, read them:
# ca_agencies_new <- read_csv("...") ; ca_doj_data <- read_csv("...")

# 1) Helper: normalize/clean agency names for better fuzzy matching
clean_agency <- function(x) {
  x %>%
    str_to_upper() %>%
    str_replace_all("&", " AND ") %>%
    str_replace_all("[^A-Z0-9 ]", " ") %>%           # drop punctuation
    str_replace_all("\\s+", " ") %>%                 # collapse spaces
    # remove common suffixes/words that differ by source
    str_replace_all("\\b(POLICE|DEPARTMENT|DEPT|DIVISION|BUREAU|OFFICE)\\b", "") %>%
    str_replace_all("\\b(SHERIFFS?|SHERIFFS OFFICE|SO)\\b", "SHERIFF") %>%
    str_replace_all("\\b(CITY OF|COUNTY OF|CITY|COUNTY|TOWN OF|TOWN)\\b", "") %>%
    str_trim()
}

# 2) Prepare data: keep only the needed columns and de-dup NCIC values
left_tbl <- ca_agencies_new %>%
  mutate(name_clean = clean_agency(name)) %>%
  filter(!is.na(name_clean) & name_clean != "")

right_tbl <- ca_doj_data %>%
  transmute(NCICCode = as.character(NCICCode)) %>%
  distinct(NCICCode) %>%
  mutate(ncic_clean = clean_agency(NCICCode)) %>%
  filter(!is.na(ncic_clean) & ncic_clean != "")

# 3) Fuzzy match:
#    - Jaroâ€“Winkler distance (good for short strings, transpositions)
#    - max_dist controls how far we allow; 0.15â€“0.20 is a practical starting range
matches_all <- stringdist_left_join(
  left_tbl, right_tbl,
  by = c("name_clean" = "ncic_clean"),
  method = "jw",
  max_dist = 0.15,
  distance_col = "distance"
) %>%
  mutate(similarity = 1 - distance) %>%              # convert distance to similarity (1 = perfect)
  arrange(name, distance)

# 4) Best single match per agency name
matches_best <- matches_all %>%
  group_by(ori, name) %>%
  slice_min(distance, n = 1, with_ties = FALSE) %>%
  ungroup()

# 5) (Optional) Top 3 candidates per agency for manual review
matches_top3 <- matches_all %>%
  group_by(ori, name) %>%
  slice_min(distance, n = 3, with_ties = FALSE) %>%
  ungroup()

# Quick peeks
matches_best %>% select(ori, name, NCICCode, similarity) %>% head(10)
matches_top3 %>% select(ori, name, NCICCode, similarity) %>% head(10)


```
subset the ca_doj_data df to just records to review from new cde
```{r}
# Filter ca_doj_data to keep only NCICCodes that appear in matches_best
ca_doj_matched <- ca_doj_data %>%
  semi_join(matches_best, by = "NCICCode")

# Quick check
nrow(ca_doj_matched)
length(unique(ca_doj_matched$NCICCode))


```
clean up the CA DOJ data
```{r}
library(dplyr)
library(stringr)

# 1) Filter to Year >= 2017
ca_doj_matched_sub <- ca_doj_matched %>%
  filter(Year >= 2017)

# 2) Keep columns only up to "HomicideClr_sum"
keep_cols <- names(ca_doj_matched_sub)[
  1:which(names(ca_doj_matched_sub) == "HomicideClr_sum")
]

ca_doj_matched_sub <- ca_doj_matched_sub %>%
  select(all_of(keep_cols))

# 3) Remove "_sum" from column names
names(ca_doj_matched_sub) <- str_remove(names(ca_doj_matched_sub), "_sum$")

# Quick check
glimpse(ca_doj_matched_sub)

ca_doj_matched_sub <- ca_doj_matched_sub %>% select(-ViolentClr, -HomicideClr)

#add back in name and ori
# Join ca_doj_matched_sub with matches_best on NCICCode
ca_doj_matched_sub <- ca_doj_matched_sub %>%
  left_join(
    matches_best %>% select(NCICCode, ori, name),
    by = "NCICCode"
  )

# Quick check
glimpse(ca_doj_matched_sub)

# Reorder columns so ori and name come first
ca_doj_matched_sub <- ca_doj_matched_sub %>%
  relocate(ori, name, .before = Year)

# Quick check
glimpse(ca_doj_matched_sub)


#subset to 2021
ca_doj_matched_sub <- ca_doj_matched_sub %>% filter(Year == 2021)
#write it out

# Define output path
output_path <- "C:/OneDrive/OneDrive - ahdatalytics.com/Clients/Real Time Crime Index/Open Source Data/Collected Sample Data/ca_doj_2021.csv"

# Write out as CSV
write_csv(ca_doj_matched_sub, output_path)
```